{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<table bgcolor=#ffffff align=\"center\" width=\"100%\" noborder>\n",
    "    <tr>\n",
    "        <td align=\"left\" width=\"30%\"><img src=\"images/IST_logo.png\" width=\"50%\"></td>\n",
    "        <td width=\"40%\"></td>\n",
    "        <td align=\"right\" width=\"30%\"><img src=\"images/ds_logo.png\" width=\"25%\">\n",
    "    </tr>\n",
    "</table>\n",
    "<h1 align=\"center\" style=\"font-family:Arial;color:#00004d;font-size:40px;\">Data Science Labs</h1>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2 align=\"center\" style=\"font-family:Arial;color:#6c6c6c;font-size:30px;\">Lab 4: Evaluation</h2>\n",
    "\n",
    "The evaluation of the results of each learnt model, in the classification paradigm, is objective and straightforward.\n",
    "We just need to assess if the predicted labels are correct, which is done by measuring the number of records where \n",
    "the predicted label is equal to the known ones.\n",
    "\n",
    "<h4 style=\"font-family:Arial;color:#6c6c6c;font-size:20px;font-style:italic;\">Accuracy</h4>\n",
    "\n",
    "The simplest measure is <strong>accuracy</strong>, which reports the percentage of correct predictions. It is just the\n",
    "opposite of <strong>error</strong>. In <code>sklearn</code>, accuracy is reported through the <code>score</code> method\n",
    "from each classifier, after its training and measured over a particular dataset and its known labels."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "0.7489177489177489"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 5
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "data = pd.read_csv('data/diabetes.csv')\n",
    "y = data.pop('class').values\n",
    "X = data.values\n",
    "\n",
    "trnX, tstX, trnY, tstY = train_test_split(X, y, train_size=0.7, stratify=y)\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(trnX, trnY)\n",
    "clf.score(tstX, tstY)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In our example, naive Bayes presents a not so good, only above 70%. But by itself, this number doesn't allow for\n",
    "understanding which are the errors - where Naive Bayes is struggling.\n",
    "\n",
    "<h4 style=\"font-family:Arial;color:#6c6c6c;font-size:20px;font-style:italic;\">Confusion Matrix</h4>\n",
    "\n",
    "Indeed, first we need to distinguish among the errors. In the presence of <strong>binary classification</strong>\n",
    "problems (there are only two possible outcomes for the target variable), this distinction is easy. Usually, we call  \n",
    "<i>negative</i> to the most common target value and <i>positive</i> to the other one. \n",
    "\n",
    "From this, we have:\n",
    "- <strong>true positives (TP)</strong>: the number of positive records rightly predicted as positve;\n",
    "- <strong>true negatives (TN)</strong>: the number of negatives records rightly predicted as negative;\n",
    "- <strong>false positives (FP)</strong>: the number of negative records wrongly predicted as positve;\n",
    "- <strong>false negatives (FN)</strong>: the number of positive records wrongly predicted as negative.\n",
    "\n",
    "The <strong>confusion matrix</strong> is the standard to present these numbers, and is computed through the \n",
    "<code>confusion matrix</code> in the <code>sklearn.metrics</code> package. "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 48,  33],\n       [ 25, 125]])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 6
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "labels: np.ndarray = pd.unique(y)\n",
    "prdY: np.ndarray = clf.predict(tstX)\n",
    "cnf_mtx: np.ndarray = metrics.confusion_matrix(tstY, prdY, labels)\n",
    "cnf_mtx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Unfortunately, we are not able to see the correspondence between the matrix elements and the enumerated above, since \n",
    "there is no standard to specify the row and columns in the matrix. So the best way is to plot it in a specialized \n",
    "chart, as the one below."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Whenever in the presence of non-binary classification, we adapt those notions for each possible combination. For \n",
    "example, in the <i>iris</i> dataset we have 3 different classes: <i>iris-setosa</i>, <i>iris-versicolor</i> and \n",
    "<i>iris-virginica</i>."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels: np.ndarray = pd.unique(y)\n",
    "prdY: np.ndarray = clf.predict(tstX)\n",
    "cnf_mtx: np.ndarray = metrics.confusion_matrix(tstY, prdY, labels)\n",
    "cnf_mtx\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Besides the confusion matrix, there are several other measures that try to reflect the quality of the model, also \n",
    "available in the <code>sklearn.metrics</code> package. Next, we summarize the most used.\n",
    "\n",
    "<table noborder>\n",
    "    <tr><th><p text-align:\"center\">Classification metrics</th></tr>\n",
    "    <tr><th><p text-align:\"center\"></th></tr>\n",
    "    <tr><td><p text-align:\"left\"><code><b>recall_score(tstY: np.ndarray, prdY: np.ndarray) -> [0..1]</b></code></td></tr>\n",
    "    <tr><td><p text-align:\"left\">also called <i>sensitivity></i> and <i>TP rate</i>, reveals the models ability to\n",
    "        recognize the positive records, and is given by <code>TP/(TP+FN)</code>; receives the known labels in \n",
    "        <i>tstY</i> and the predicted ones in <i>prdY</i></td></tr>\n",
    "    <tr><td><p text-align:\"left\"><code><b>precision_score(tstY: np.ndarray, prdY: np.ndarray) -> [0..1]</b></code></td></tr>\n",
    "    <tr><td><p text-align:\"left\">reveals the models ability to not misclassify negative records, and is given by \n",
    "        <code>TP/(TP+FP)</code>; receives the known labels in <i>tstY</i> and the predicted ones in <i>prdY</i></td></tr>\n",
    "    <tr><td><p text-align:\"left\"><code><b>f1_score(tstY: np.ndarray, prdY: np.ndarray) -> [0..1]</b></code></td></tr>\n",
    "    <tr><td><p text-align:\"left\">computes the average between precision and recall, and is given by \n",
    "        <code>2 * (precision * recall) / (precision + recall)</code>; receives the known labels in <i>tstY</i> \n",
    "        and the predicted ones in <i>prdY</i></td></tr>\n",
    "    <tr><td><p text-align:\"left\"><code><b>balanced_accuracy_score(tstY: np.ndarray, prdY: np.ndarray) -> [0..1]</b></code></td></tr>\n",
    "    <tr><td><p text-align:\"left\">reveals the average of recall scores for all the classes; receives the known \n",
    "        labels in <i>tstY</i> and the predicted ones in <i>prdY</i></td></tr>\n",
    "</table>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h4 style=\"font-family:Arial;color:#6c6c6c;font-size:20px;font-style:italic;\">ROC Charts</h4>\n",
    "\n",
    "<strong>ROC charts</strong> are another mean to understand models' performance, in particular in the presence of binary non balanced \n",
    "datasets.\n",
    "They present the balance between True Positive rate (recall) and False Positive rate in a graphical way, and are\n",
    "available through the <code>roc_curve</code> method in the <code>sklearn.metrics</code>.\n",
    "\n",
    "In addition to the chart, the <i>area under the roc curve</i> is another important measure, mostly for unbalanced \n",
    "datasets. It is available as <code>roc_auc_score</code>, like for other scores, receives the known labels in its first\n",
    "parameter and the predicted ones in the second. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<table bgcolor=#ffffff align=\"center\" width=\"100%\" noborder>\n",
    "    <tr>\n",
    "        <td align=\"center\" width=\"30%\"><a href=\"Lab30_classification.ipynb\"><img src=\"images/prev.png\"></a></td>\n",
    "        <td width=\"40%\"></td>\n",
    "        <td align=\"center\" width=\"30%\"><a href=\"Lab3.ipynb\"><img src=\"images/next_grey.png\"></a></td>\n",
    "    </tr>\n",
    "</table>"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}